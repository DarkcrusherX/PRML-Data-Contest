# -*- coding: utf-8 -*-
"""prml.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11t2eTyOTiY943OYjbJRxXIjcxCXsCubT
"""

import numpy as np 
import sklearn
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn import preprocessing
from sklearn.metrics import mean_squared_error
import csv
from tqdm import tqdm

from sklearn import datasets, ensemble
from sklearn.inspection import permutation_importance
import xgboost as xg
from sklearn.ensemble import AdaBoostRegressor

from google.colab import drive
drive.mount('/content/drive')

## Read the data
data = pd.read_csv("/content/drive/My Drive/Dataset/train.csv")
test = pd.read_csv("/content/drive/My Drive/Dataset/test.csv")

## Encode the user ids
le = preprocessing.LabelEncoder()
le.fit(data['customer_id'].unique())
data['customer_id'] = le.transform(data['customer_id'])
test['customer_id'] = le.transform(test['customer_id'])

## Train-val split
train, val = train_test_split(data, test_size=0.2, random_state=1)
# val = data[1:1000]

## Extra info 

### Songs df
songs_df = pd.read_csv("/content/drive/My Drive/Dataset/songs.csv")
songs_df.loc[songs_df['released_year']<=0 ,'released_year'] = np.nan

### Songs labels
songs_labels = pd.read_csv("/content/drive/My Drive/Dataset/song_labels.csv")
# songs_labels['platform_id'] = le.transform(songs_labels['platform_id'])

### Saved later songs df
songs_saved_later_df = pd.read_csv("/content/drive/My Drive/Dataset/save_for_later.csv")
songs_saved_later_df['customer_id'] = le.transform(songs_saved_later_df['customer_id'])

a = songs_df['released_year'].isna().sum()

a

songs_df['language'] = songs_df['language'].fillna('enn')
songs_df['released_year'] = songs_df['released_year'].fillna('2000')

songs_df

## Encode the user ids
le2 = preprocessing.LabelEncoder()
le2.fit(songs_df['language'].unique())
songs_df['language'] = le2.transform(songs_df['language'])

Train_Dataset = pd.merge(train,songs_df,on="song_id")
# Train_Dataset = pd.merge(Train_Dataset_sub1,songs_labels,on="platform_id")

Test_Dataset = pd. merge(test,songs_df,on="song_id")
# Test_Dataset = pd.merge(Test_Dataset_sub1,songs_labels,on="platform_id")

Valid_Dataset = pd. merge(val,songs_df,on="song_id")
# Valid_Dataset = pd.merge(Valid_Dataset_sub1,songs_labels,on="platform_id")

Train_Dataset[1:10]

Valid_Dataset.drop('platform_id',inplace=True, axis=1)

Test_Dataset.drop('platform_id',inplace=True, axis=1)

Train_Dataset.drop('platform_id',inplace=True, axis=1)

# column_names = ["customer_id", "song_id", "released_year","language","number_of_comments"]

# Valid_Dataset = Valid_Dataset.reindex(columns=column_names)

# Test_Dataset = Test_Dataset.reindex(columns=column_names)

# Train_Dataset = Train_Dataset.reindex(columns=column_names)

Train_Dataset

params = {'n_estimators': 500,
          'max_depth': 10,
          'min_samples_split': 3,
          'learning_rate': 0.1,
          'loss': 'ls'}

Train_Score = Train_Dataset['score']
Valid_Score = Valid_Dataset['score']

Train_Dataset.drop('score',inplace=True,axis = 1)
Valid_Dataset.drop('score',inplace=True,axis = 1)

Train_Dataset.drop('language',inplace=True,axis = 1)
Valid_Dataset.drop('language',inplace=True,axis = 1)
Test_Dataset.drop('language',inplace=True,axis = 1)

Train_Score

Train_Dataset['released_year'] = pd.to_numeric(Train_Dataset['released_year'])
Valid_Dataset['released_year'] = pd.to_numeric(Valid_Dataset['released_year'])
Test_Dataset['released_year'] = pd.to_numeric(Test_Dataset['released_year'])

# reg3 = ensemble.GradientBoostingRegressor(**params,verbose= 2)
reg = xg.XGBRegressor(objective ='reg:linear',n_estimators = 125,booster='gbtree', seed = 123,max_depth=17,verbosity=3)
# reg =  AdaBoostRegressor(random_state=0, n_estimators=150,learning_rate = 1)
# reg = ensemble.RandomForestRegressor(random_state=1,n_estimators=100,min_samples_split=8,max_depth=20,verbose = 2)
# reg = ensemble.VotingRegressor(estimators=[('xgb', reg1), ('rf', reg2), ('gb',reg3)])

reg.fit(Train_Dataset, Train_Score)

Train_Dataset.dtypes

mse = mean_squared_error(Valid_Score, reg.predict(Valid_Dataset))
mse1 = mean_squared_error(Train_Score, reg.predict(Train_Dataset))

mse

mse1